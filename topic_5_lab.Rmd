---
title: "Topic 5: Word Relationships"
output: pdf_document
---

```{r packages, warning=FALSE, message=FALSE, include=FALSE}
library(tidyr) #text analysis in R
library(pdftools)
library(lubridate) #working with date data
library(tidyverse)
library(tidytext)
library(readr)
library(quanteda)
library(readtext) #quanteda subpackage for reading pdf
library(quanteda.textstats)
library(quanteda.textplots)
library(ggplot2)
library(forcats)
library(stringr)
library(quanteda.textplots)
library(widyr)# pairwise correlations
library(igraph) #network plots
library(ggraph)
library(here)

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

### import EPA EJ Data
```{r pdf_import}
#setwd(here('data/'))

files <- list.files(path = here("data_EPA/"),
                    pattern = "EPA*", full.names = TRUE)

ej_reports <- lapply(files, pdf_text)


ej_pdf <- readtext(file = here("data_EPA", "EPA*"), 
                   docvarsfrom = "filenames", 
                   docvarnames = c("type", "subj", "year"),
                   sep = "_")

#creating an initial corpus containing our data
epa_corp <- corpus(x = ej_pdf, text_field = "text")
summary(epa_corp)

# adding additional, context-specific stop words to stop word lexicon
more_stops <-c("2015","2016", "2017", "2018", "2019", "2020", "www.epa.gov", "https")
add_stops<- tibble(word = c(stop_words$word, more_stops)) 
stop_vec <- as_vector(add_stops)

```


### create different data objects to set up for subsequent analyses
```{r tidy}
#convert to tidy format and apply stop words
raw_text <- tidy(epa_corp)

#Distribution of most frequent words across documents
raw_words <- raw_text %>%
  mutate(year = as.factor(year)) %>%
  unnest_tokens(word, text) %>%
  anti_join(add_stops, by = 'word') %>%
  count(year, word, sort = TRUE)

#number of total words by document  
total_words <- raw_words %>% 
  group_by(year) %>% 
  summarize(total = sum(n))

report_words <- left_join(raw_words, total_words)
 
par_tokens <- unnest_tokens(raw_text, output = paragraphs, input = text, token = "paragraphs")

par_tokens <- par_tokens %>%
 mutate(par_id = 1:n())

par_words <- unnest_tokens(par_tokens, output = word, input = paragraphs, token = "words")

```


### quanteda
```{r quanteda_init}
tokens <- tokens(epa_corp, remove_punct = TRUE)
toks1<- tokens_select(tokens, min_nchar = 3)
toks1 <- tokens_tolower(toks1)
toks1 <- tokens_remove(toks1, pattern = (stop_vec))
dfm <- dfm(toks1)

#first the basic frequency stat
tstat_freq <- textstat_frequency(dfm, n = 5, groups = year)
head(tstat_freq, 10)

```


### bigrams
```{r convert_dfm}
# bigrams 

toks2 <- tokens_ngrams(toks1, n=2)
dfm2 <- dfm(toks2)
dfm2 <- dfm_remove(dfm2, pattern = c(stop_vec))
freq_words2 <- textstat_frequency(dfm2, n=20)
freq_words2$token <- rep("bigram", 20)
#tokens1 <- tokens_select(tokens1,pattern = stopwords("en"), selection = "remove")
```


### Assignment

1.  What are the most frequent trigrams in the dataset? How does this compare to the most frequent bigrams? Which n-gram seems more informative here, and why?
```{r}
toks3 <- tokens_ngrams(toks1, n=3)
dfm3 <- dfm(toks3)
dfm3 <- dfm_remove(dfm3, pattern = c(stop_vec))
freq_words3 <- textstat_frequency(dfm3, n=20)
freq_words3$token <- rep("trigram", 20)
``` 

**The most frequent trigrams in the dataset are justice_fy2017_progress, fy2017_progress_report, environmental_public_health, environmental_justice_fy2017, and national_environmental_justice, followed by more trigrams including more variations of environmental justice. All of the trigrams have a frequency of 51 or less. In comparison, the most frequent bigram is environmental_justice with a frequency of 556, followed by technical_assistance, drinnking_water, public_health, and progress_report, which are all between 108-139, far behind environmental_justice. Because environmental_justice is used so frequently as seen in the bigram dataframe, I would say the trigrams are more informative because the various contexts of how environmental justice is used is more clearly highlighted when adding the extra word (i.e., environmental justice progress, environmental justice grants, communities environmental justice, etc.).**

2.  Choose a new focal term to replace "justice" and recreate the correlation table and network (see corr_paragraphs and corr_network chunks). Explore some of the plotting parameters in the cor_network chunk to see if you can improve the clarity or amount of information your plot conveys. Make sure to use a different color for the ties!
```{r}
word_cors_2 <- par_words %>% 
  add_count(par_id) %>% 
  filter(n >= 50) %>% 
  select(-n) %>%
  pairwise_cor(word, par_id, sort = TRUE)

indig_cors <- word_cors_2 %>% 
  filter(item1 == "indigenous")  

  word_cors_2 %>%
  filter(item1 %in% c("environmental", "indigenous", "equity", "income"))%>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item1 = as.factor(item1),
  name = reorder_within(item2, correlation, item1)) %>%
  ggplot(aes(y = name, x = correlation, fill = item1)) + 
  geom_col(show.legend = FALSE) +
  facet_wrap(~item1, ncol = 2, scales = "free")+
  scale_y_reordered() +
  labs(y = NULL,
         x = NULL,
         title = "Correlations with key words",
         subtitle = "EPA EJ Reports")
  
  #let's zoom in on just one of our key terms
   indigenous_cors <- word_cors_2 %>% 
  filter(item1 == "indigenous") %>%
   mutate(n = 1:n())

```

```{r}
indigenous_cors  %>%
  filter(n <= 35) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "purple") +
  geom_node_point(size = 3) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

**I chose the word "indigenous" for this exercise. When creating the network I decided to modify the color parameter, decreased the node size, and also filtered for only the top 35 words in order to reduce some of the crowdedness of the plot.**

3.  Write a function that allows you to conduct a keyness analysis to compare two individual EPA reports (hint: that means target and reference need to both be individual reports). Run the function on 3 pairs of reports, generating 3 keyness plots.

```{r}
# make new dfm with just two reports so that the target is one doc and the reference is another single doc

# two different years will be the two parameters
# before making corpus, filter for only the two relevant years 
# use one year as the target_year and the other will automatically be the reference_year

keyness_function <- function(target_year, reference_year){
  files <- list.files(path = here("data_EPA/"),
                    pattern = "EPA*", full.names = TRUE)
  
  ej_reports <- lapply(files, pdf_text)
  ej_pdf <- readtext(file = here("data_EPA", "EPA*"), 
                   docvarsfrom = "filenames", 
                   docvarnames = c("type", "subj", "year"),
                   sep = "_")
  
  ej_pdf_filtered <- ej_pdf %>%
    filter(year == target_year | year == reference_year)
    

# creating an initial corpus containing the data
epa_corp <- corpus(x = ej_pdf_filtered, text_field = "text")

# adding some additional, context-specific stop words to stop word lexicon
more_stops <-c("2015","2016", "2017", "2018", "2019", "2020", "www.epa.gov", "https")
add_stops<- tibble(word = c(stop_words$word, more_stops))
stop_vec <- as_vector(add_stops)

tokens <- tokens(epa_corp, remove_punct = TRUE)
toks1<- tokens_select(tokens, min_nchar = 3)
toks1 <- tokens_tolower(toks1)
toks1 <- tokens_remove(toks1, pattern = (stop_vec))
dfm <- dfm(toks1)

keyness <- textstat_keyness(dfm, target = 1) # target = 1 is the first report, dfm2 = bigrams
return(textplot_keyness(keyness))

}
```

```{r}
# produce 3 keyness plots 

keyness_function(2015, 2016)

keyness_function(2017, 2018)

keyness_function(2019, 2020)

```

 
4.  Select a word or multi-word term of interest and identify words related to it using windowing and keyness comparison. To do this you will create two objects: one containing all words occurring within a 10-word window of your term of interest, and the second object containing all other words. Then run a keyness comparison on these objects. Which one is the target, and which the reference? [Hint](https://tutorials.quanteda.io/advanced-operations/target-word-collocations/)
```{r}
# use toks1

pollution <- c("pollution", "pollut*")

toks_inside <- tokens_keep(toks1, pattern = pollution, window = 10)

toks_inside <- tokens_remove(toks1, pattern = pollution) # remove the keywords

toks_outside <- tokens_remove(toks1, pattern = pollution, window = 10)
```

```{r}
# compute wordsâ€™ association with the keywords using textstat_keyness
dfmat_inside <- dfm(toks_inside)
dfmat_outside <- dfm(toks_outside)

tstat_key_inside <- textstat_keyness(rbind(dfmat_inside, dfmat_outside), 
                                     target = seq_len(ndoc(dfmat_inside)))
head(tstat_key_inside, 50)

textplot_keyness(tstat_key_inside)
```

**I chose the word pollution as my word of interest. When running the keyness comparison, the object including the words occurring within a 10-word window is the target object (dfmat_inside), and the object containing all words outside of the window is the reference object (dfmat_outside).**

